---
title: "Social-Media und Text-Mining-Workshop mit R"
output: html_notebook
---
#### 1. Grundlegende Schritte der Textanalyse

A) **Erhebung von Textdaten**
- Bestehende Archive (z.B. [Reddit Pushift Data](https://files.pushshift.io/reddit/comments/), vgl. auch [Baumgartner et al., 2020](https://cs.paperswithcode.com/paper/the-pushshift-reddit-dataset))
- APIs (z.B. [The Guardian](https://cran.r-project.org/web/packages/guardianapi/guardianapi.pdf))
- Web-Scraping (z.B. mit [rvest](https://rvest.tidyverse.org/)) (oder just for fun, build you own API mit [plumber](https://www.rplumber.io/))

B) **Datenaufbearbeitung** (*Pre-processing*)
 - Bereinigung
- Auswahl und Gewichtung von *Features* (Dimensionsreduktion, Filtern von häufigen/ seltenen Wörtern)

C) **Analyse**
- *Dictionary*- oder regelbasierte Ansätze
- Supervised Ansätze (z.B. SVM)
- Unsupervised Ansätze (z.B. Topic Modeling)

D) **Validierung**
- z.B. Reliabilität von Bot-Klassifizierung ([tweetbotornot2](https://github.com/mkearney/tweetbotornot2#botometer))


#### 2. Datenzugänge

A) **Screen-Scraping**: 
- Auslesen des HTML-Codes aus dem Browser 
- *Scraping*, *Parsing* und Formatierung (z.B. mit *Rselenium*)


B) **API-Access-Points**: 
- Anfragen direkt an die Datenbank senden
- Gateways für den Zugang zu definierten Datentypen für verschiedene Parteien, unabhängig von der Kodierungssprache


##### B.1. Application Programming Interfaces (APIs)

- kommuniziert direkt mit der Datenbank
- steuert, *welche* Informationen zugänglich sind, für *wen*, *wie* und in welchen *Mengen*


B.2. **API-Anwendungen**:

- Inhalte in andere Anwendungen einbetten
- Bots erstellen (z.B. [Telegram](https://core.telegram.org/bots/api))
- Daten für (Markt-)Forschungszwecke sammeln

Solche Zugangspunkte bestehen für:

- [**Youtube**](https://developers.google.com/youtube/v3) 
-- erlaubt es durch Keywords nach Inhalten, dem Video, den Wiedergabelisten und Nutzeraktivitäten wie Upvoting, Kommentare, Kommentaren, Favorisieren zu suchen 

- [**Instagram**](https://github.com/digitalmethodsinitiative/dmi-instascraper/) 
-- erlaubt es Kommentarstrukturen im Zusammenhang mit Postings, Freundschaftsinformationen von Nutzern oder Geolokation zu erhalten

- [**Wikipedia**](https://cran.r-project.org/web/packages/WikipediR/WikipediR.pdf)
-- erlaubt es MediaWiki Revisionen, die mit dem Eintrag verbundene Bearbeitungszusammenfassung, Zeitstempel, Seiteninformationen, Nutzerinformationen zu erhalten

- [**Google Maps**](https://cran.rstudio.com/web/packages/mapsapi/vignettes/intro.html)
-- erlaubt es Koordinaten für Breiten- und Längengrade, Distanzmatrix zu erhalten



B.3. **Vorteile von API-Harvesting**:

- Keine Interaktion mit HTML-Dateien erforderlich (Output: *JSON*-Dateien)
- Zumeist legal (einhalten von Nutzungsbedingungen und Datenschutz- und Urheberrechtsbestimmungen beachten)


B.4. **Nachteile des API-Harvesting**:

- Nicht jede Website hat eine API
- Nur die Daten abrufbar, die API zur Verfügung stellt
- *Rate-Limitationen* (z.B. Anzahl der Tweets pro Tag/ Query)
- Nutzungsbedingungen und Veränderungen an der API schränken Nutzung ein (z.B. Code-Reprodzierbarkeit, Teilen von Datensätzen)
- Code variiert je nach Plattform und Detailierungsgrad der Dokumentation


#### 3. Tool-Übersicht zum Twitter Datenzugang
Twitter für Forschung zur Dynamik schnelllebiger sozio-politischer Ereignisse und zeitgenössischer Kultur

Unterscheidung nach Anforderungen:

- Graphical User Interface
- Art der Daten
- Eignung zur Erhebung oder auch zur Aufbereitung und Analyse
- API Version (z.b. *Twitter API v1.1* bieten das Pakete `rtweet`)

**Quellen zu Tools**: 

- [Twitter Tool List im Wiki des Social Media Observatory am Leibniz-Institut für Medienforschung | Hans-Bredow-Institut (HBI)](https://smo-wiki.leibniz-hbi.de/Twitter-Tools)

- Tutorial zu Twitter-Daten: Jürgens, P., & Jungherr, A. (2016). A tutorial for using Twitter data in the social sciences: Data collection, preparation, and analysis. Preparation, and Analysis (January 5, 2016).)

- [Tools zur Datenerhebung und Analyse von Social-Media-Daten](https://wiki.digitalmethods.net/Dmi/ToolDatabase)


#### 4. Academic Twitter Access Point (v2 API endpoints)
- Um die folgenden Code-Beispiele ausführen zu können, wird [Academic Research Access](https://developer.twitter.com/en/products/twitter-api/academic-research) für die Twitter API v2 benötigt. Zum Sampling von Twitter Daten nutzt man das `R`-Paket [`academictwitteR`](https://cran.r-project.org/web/packages/academictwitteR/academictwitteR.pdf).

- In dem Rahmen benötigt man den sogenannten [Bearer Token](https://developer.twitter.com/en/docs/authentication/oauth-2-0/bearer-tokens). In dieser [Vignette](https://cran.r-project.org/web/packages/academictwitteR/vignettes/academictwitteR-auth.html) im `academictwitteR`-Paket wird der Vorgang wie man Zugang zur Twitter API erhält erklärt.

**Quellen zur Academic Twitter API**:

- https://developer.twitter.com/en/use-cases/do-research/academic-research/resources
- https://cran.r-project.org/web/packages/academictwitteR/vignettes/academictwitteR-tidy.html
- https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query#list

#### 4.1. Twitter Modi mit v2 API Endpoints

```{r Twitter academic code für tweets, warning=FALSE}
# Tweet-zentriert-Archiv
get_all_tweets(
  query = 'xyz',
  start_tweets,
  end_tweets,
  bearer_token = get_bearer(),
  n = 1000
)
```

```{r Twitter academic code für User_Id, warning=FALSE}
# User_ID erhalten--Nutzer-zentriert

users <- c("juliasilge", "drob")

get_user_id(users, bearer_token)
```

```{r Twitter academic code für User Timeline}
# Nutzer-zentriert--Timeline Crawling
get_user_timeline(
userid, #single string oder Vektor mit User_Ids
start_tweets,
end_tweets,
bearer_token = get_bearer(),
n = 100,
)
```


#### 5. Demo-Datenquelle

**1: [Kaggle Data Dump, Ukraine, "en_tweets_sample.pkl"](https://www.kaggle.com/code/josbenard/prepare-datasets/data)**

(2: [Kaggle Data Dump, Squid Game, "tweets_v8.csv"](https://www.kaggle.com/datasets/deepcontractor/squid-game-netflix-twitter-data))

#### `R`-Pakete
```{r, eval=FALSE}
library(academictwitteR)    # Datenerhebung
library(dplyr)              # Data Wrangling, Aufbearbeitung
library(quanteda)           # Text Mining
library(quanteda.textstats)
library(quanteda.textplots)
library(tidytext)           # Text Mining im 'tidy format'
library(textclean)          # Normalisierung und Pre-Processing 
library(lubridate)          # Wrangling mit Zeitdaten
library(ggplot2)

theme_set(theme_light())
```
#### 6. Sampling
**Authentication**
```{r}
#Einlesen API Credentials
oautCred <- read.delim(file = "oautCred.txt", header = TRUE, sep = ",") %>% unlist()

#Einlesen "Bearer Token" in die Environment
Sys.setenv(TWITTER_BEARER = unname(oautCred["bearer"]))
```
**Twitter-Query**
```{r}
#Tweets Abfrage (basierend auf academic researcher developer account)
#Tweet Suchabfrage (beinhaltet "#hashtags, ggf. "keywords", Sprache)

get_all_tweets(
  query = '(#Ukraine) -is:retweet',                   #Tweet Suchabfrage
               n = Inf,                               #max tweet nr 
               start_tweets = "2022-08-19T00:00:00Z", #Start-Datum
               end_tweets = "2022-08-19T23:59:59Z",   #End-Datum
               file = "ukraine_tweets",               #Dateiname 
               data_path = "ukraine_data/",           #Pfad zur Datei
               bind_tweets = TRUE,                    #Format als Tabelle
               )


#Entfernen der API Credentials
rm(oautCred)

```
- **? Welche Implikationen im Forschungszyklus haben Entscheidungen bei der Formulierung der Query?**



```{r}
tweets <- bind_tweets(data_path = "ukraine_data/", output_format = "tidy")
```
#### 7. Überblick über die Struktur der Twitter-Daten

```{r}
glimpse(tweets)
```
**Anteil der Sprachen**
Wie verteilen sich die Sprachen über die Accounts?
```{r Sprachverteilung}
tweets %>% 
  group_by(language) %>% 
  count() %>% 
  arrange(desc(n))
```
**Verteilung der Tweets per minute**
```{r Verteilung Tweet Anzahl}
ggplot(data = tweets,
      aes(x = tweetcreatedts)) +
      geom_histogram(bins = 57, color = "black", fill = NA)+
      ggtitle(label = "Tägliche Tweetanzahl")
```
**Parsing der DAte-Time Variables mit [lubridate](https://lubridate.tidyverse.org/)**
```{r Wrangling Zeitvariablen}
tweets <- tweets %>%
  mutate(minute = (floor_date(tweetcreatedts, 
                           unit = "minute"))) #Runden von date-time Objekten (Sekunde, Woche, Monat)

```
*Tweets und Retweets per minute** 
```{r}
minute_summary <- tweets %>%
  group_by(minute) %>%
  summarize(tweets = n(),
            avg_retweets = exp(mean(log(retweetcount + 1))) - 1)

minute_summary %>%
  ggplot(aes(minute, tweets)) +
  geom_line(color = "darkblue", size = 0.5) +
  expand_limits(y = 0) +
  labs(x = "Time",
       y = "Number of #Ukraine tweets per hour")

minute_summary %>%
  ggplot(aes(minute, avg_retweets)) +
  geom_line(color = "darkblue", size = 0.5) +
  expand_limits(y = 0) +
  labs(x = "Time",
       y = "Average (geometric mean) retweets each hour")
```
**Twitter Aktivität per User**
```{r}
tweets %>%
  count(username, sort = TRUE) %>%
  head(12) %>%
  mutate(username = reorder(username, n)) %>%
  ggplot(aes(username, n)) +
  geom_col() +
  coord_flip()

#Retweet Aktivität der User
tweets %>%
  arrange(desc(retweetcount)) %>%
  select(username, text, retweetcount)
#Tweets und Retweets pro User
tweets %>%
  group_by(username) %>%
  summarize(tweets = n(),
            retweets = sum(retweetcount)) %>%
  arrange(desc(tweets)) %>%
  arrange(desc(retweets))

#Anteil Likes und Retweets pro User
tweets %>%
  select(username, text, retweetcount, favorite_count) %>%
  mutate(ratio = (favorite_count + 1) / (retweetcount + 1)) %>% # to avoid zero
  arrange(desc(ratio)) %>% 
  view(n=10)

```

#### 8. Grundlagen Text Mining in a Nutshell
A.**Document** = Sammlung von Zeichenfolgen und ihrer Metadaten

B.**Corpus** = Sammlung von *Documents* 

C.**Tokens** = kleinste Bedeutungseinheit (meist Wörter)

D.**Vocabulary** = Sammlung eindeutiger Wörter eines *Corpus*

E.**D**ocument-**F**eature-**M**atrix oder **D**ocument-**T**erm-**M**atrix = 

dies ist eine Matrix, in der die Zeilen die Dokumente, die Spalten die Ausdrücke und die Zellen die Häufigkeit der Ausdrücke in den einzelnen Dokumenten angeben, d.h.: Matrix mit *n* = Anzahl der Document-Zeilen und *m* = Größe der Vocabulary Spalten, basiert auf der **bag-of-words**-Annahme, das die Reihenfolge der Wörter und die Syntax ignoriert. 



##### 8.1. Pre-processing
A. **Tokenisation**

B. **Stopwords** entfernen

C. **String** operations (Interpunktion, URL normalisieren)

Die Reihenfolge der Anwendungsschritte ist entscheidend und sollte durch die Forschungsfrage geleitet sein.

```{r Pre-processing Punctuation}
# Bereinigen von @-Symbol
tweets$text <- gsub("@\\w+", "", tweets$text)

# Bereinigen der Interpuntion
tweets$text<- gsub("[[:punct:]]", "", tweets$text)

# Bereinigen von Nummern
tweets$text <- gsub("[[:digit:]]", "", tweets$text)

# Bereinigen von Bildern
tweets$text<- gsub("pictwitter\\w+ *", "", tweets$text)
```

```{r Pre-processing html}
#Bereinigen von HTML-Notation
remove_html <- "&amp;|&lt;|&gt;" #&lt und &gt stehen für < und > und &amp für &

tweets_en <- tweets %>% 
  filter(language == "en") %>% 
  select(tweetid, text, username) %>% 
  mutate(text = stringr::str_remove_all(text, remove_html))
```

Mit [`textclean`](https://github.com/trinker/textclean) kann weitere Harmonisierung durchgeführt werden, z.B. Ersetzen von Emojis durch Wortäquivalente.

- **?** **Welche weiteren Optionen des Pre-Processings mit `textclean` gibt es und wie wirkt sich das auf die Tweet-Anzahl etc. aus?**

```{r, warning=FALSE}
tweets_en$text <- replace_emoji(tweets$text)
```
Oder Normalisieren von Wortverlängerungen (Elongation, z.B."Whyyyy")
```{r, warning=FALSE}
tweets_en$text <- replace_word_elongation(tweets$text)
```

Aus dem Data Frame kann man ein *Corpus*-Objekt erstellen, d.h. eine Sammlung von *Dokumenten* (Tweets) und deren *Metadaten*.

```{r Corpus Objekt, results='hide'}
tweets_en_corpus <- corpus(tweets_en,
                           docid_field = "tweetid",
                           text_field = "text")

summary(tweets_en_corpus)
```
In `quanteda` kann man Text in die kleinsten Bedeutungssegmente (**tokens**) aufteilen durch das Entfernen von Trennzeichen (vgl. dazu in `tidytext` `unnest_tokens(word, text)`. Zudem entfernen wir bestimmte Zeichentypen sowie sogenannte **Stopwords** (d.h., grammatikalische Wörter, die je nach Anwendung wenig semantische Bedeutung tragen). Vgl. (`stopwords()`)

```{r Tokenisation, results=FALSE}
tweets_en_tokens <- tokens(tweets_en_corpus,
                           remove_punct = TRUE,
                           remove_numbers = TRUE,
                           remove_symbols = TRUE,
                           remove_url = TRUE) %>% 
  tokens_tolower() %>% 
  tokens_remove(stopwords("english"))
```

- **?** **Versuchen Sie es mit einer eigenen entwickelten Stopwords-Liste. Wie beinflusst Ihre Stopwords-Liste die Zusammensetzung des Tweet-Corpus?**


```{r KWIC, results='hide'}
#Mit keyword-in-context kann man eine Konkordanz-Ansicht erhalten.
kw_ukraine <- kwic(tweets_en_tokens, pattern =  "ukrain*", window = 3)
tibble::view(kw_ukraine)
```

Des Weiteren kann man in `quanteda`aus dem Token-Objekt eine **Document-Feature Matrix** (DFM) erstellen, (Wörter werden als Features eines textbasierten Datensatzes behandelt).
```{r DFM, results = 'hide'}
tweets_en_dfm <- dfm(tweets_en_tokens) 

topfeatures(tweets_en_dfm, 10)
```

Absolute Worthäufigkeiten mit `quanteda.textstats`

```{r textstat frequency quanteda.textstats}
term_freq_en <- textstat_frequency(tweets_en_dfm)
head(term_freq_en, n = 10)
```

- **? Wenn Sie sich das Ergebnis ansehen, gibt es dann korpusspezifische Begriffe, die ebenfalls als *Stopwords* betrachtet werden sollten?**

- **? Wie hoch ist der Anteil der Begriffe am gesamten Vokabular, die nur einmal im Korpus vorkommen?**


```{r lexical diversity quanteda, results='hide'}
#Lexikalische Diversität
lexdiv_tweets <- textstat_lexdiv(tweets_en_dfm)

plot(lexdiv_tweets$TTR, type = "l", xaxt = "n", xlab = NULL, ylab = "TTR")
grid()
axis(1, at = seq_len(nrow(lexdiv_tweets)), labels = tweets_en_dfm$username)

```

Umwandeln einer DFM in eine **Feature Co-occurrence Matrix (FCM)** (z.B. für Word Embeddings) 
```{r}
tweets_en_fcm <- fcm(tweets_en_dfm)

topfeatures(tweets_en_fcm) # Häufigsten Co-occurences
```


Plot der Feature Co-Coccurences der FCM mit `quanteda.textplot` als Netzwerk
```{r fcm network, warning=FALSE}
feat <- names(topfeatures(tweets_en_fcm, 50))

fcmat_tweets_select <- fcm_select(tweets_en_fcm, pattern = feat, selection = "keep")
dim(tweets_en_fcm)

size <- log(colSums(dfm_select(tweets_en_fcm, feat, selection = "keep")))

set.seed(144)
textplot_network(fcmat_tweets_select, min_freq = 0.7, vertex_size = size / max(size) * 3)

```

#### 8.2. Tidy Workflow--Exploration Tweets, Retweets und Hashtags
```{r Tokenisation und Pre-processing}
#Tokenisation der Tweets (1-gram) und Pre-processing mit tidytext

tweet_words <- tweets %>%
  select(username, 
         text, 
         retweetcount, 
         favorite_count,
         tweetcreatedts, 
         tweetid,
         hour) %>%
  unnest_tokens(word, text, token = "tweets") %>%
  anti_join(stop_words, by = "word") %>%
  filter(!word %in% c("amp", "de", "|", "la", "en", "die", "#ukraine", "und", "der", "le", "di"),
         stringr::str_detect(word, "[a-z]"))


tweet_words %>%
  count(word, sort = TRUE) %>%
  head(16) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot2::ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() +
  labs(title = "Most common words in Tweets on Ukraine",
       y = "Frequency of words")

```

```{r Favorites und Retweets, warning=FALSE}
#Verteilung Favorites
tweets %>%
  ggplot2::ggplot(aes(favorite_count + 1)) +
  geom_histogram() +
  scale_x_log10()

#Durchschnittliche Retweets und Durschnittliche Favorites
word_summary <- tweet_words  %>%
  group_by(word) %>%
  summarize(n = n(),
            avg_retweets = exp(mean(log(retweetcount + 1))) - 1,
            avg_favorites = exp(mean(log(favorite_count + 1))) - 1) %>%
  filter(n >= 30) %>%
  arrange(desc(avg_retweets))

```


```{r tf-idf}
#Tf-idf: die Kennzahl tf-idf gibt an, wie wichtig ein Wort für ein Dokument in einer Sammlung (oder einem Korpus) von Dokumenten ist
#Häufigkeit eines Begriffs, bereinigt darum, wie selten er verwendet wird
top_word <- tweet_words %>%
  count(word, hour) %>%
  bind_tf_idf(word, hour, n) %>%
  arrange(desc(tf_idf)) %>%
  distinct(hour, .keep_all = TRUE) %>%
  arrange(hour)

tf_idf <-word_summary %>%
  inner_join(top_word, by = c("word")) %>%
  arrange(desc(avg_retweets)) 

#Tf-idf der Wörter pro Stunde
tf_idf %>% 
  ggplot2::ggplot(aes(tf_idf, forcats::fct_reorder(word, tf_idf), fill = hour)) +
  geom_col(show.legend = FALSE)
```

```{r hashtags, warning=FALSE}
#Hashtag Exploration
tweet_words <- tweets %>%
  mutate(hashtags = stringr::str_count(text, "#[a-zA-Z]"), sort = TRUE) %>%
  select(username, text, retweetcount, favorite_count, tweetcreatedts , tweetid,
         hour, hashtags) %>%
  unnest_tokens(word, text, token = "tweets") %>%
  anti_join(stop_words, by = "word") %>%
  filter(!word %in% c("de", "|"),
         stringr::str_detect(word, "[a-z]"))


tweets %>%
  mutate(hashtags = stringr::str_count(text, "#[a-zA-Z]"), sort = TRUE) %>%
  filter(hashtags < 6) %>%
  group_by(username) %>%
  summarize(tweets = n(),
            avg_retweets = exp(mean(log(retweetcount + 1))) - 1) %>%
  filter(tweets >= 30) %>%
  arrange(desc(avg_retweets))


tweet_word_summary <- tweet_words %>%
  filter(hashtags < 6) %>%
  group_by(word) %>%
  summarize(n = n(),
            avg_retweets = exp(mean(log(retweetcount + 1))) - 1,
            avg_favorites = exp(mean(log(favorite_count + 1))) - 1)



tweet_word_summary %>%
  filter(n >= 100,
         !stringr::str_detect(word, "https")) %>%
  arrange(desc(avg_retweets)) %>%
  View()

tweet_word_summary %>%
  filter(n >= 100,
         !stringr::str_detect(word, "https")) %>%
  ggplot2::ggplot(aes(n, avg_retweets)) +
  geom_point() +
  geom_text(aes(label = word), check_overlap = TRUE) +
  scale_x_log10() +
  scale_y_log10()

tweet_word_summary %>%
  filter(n >= 100,
         !stringr::str_detect(word, "https")) %>%
  arrange(desc(avg_retweets)) %>%
  head(20) %>%
  mutate(word = reorder(word, avg_retweets)) %>%
  ggplot2::ggplot(aes(word, avg_retweets)) +
  geom_col() +
  coord_flip() +
  labs(title = "Which words get the most retweets in #Ukraine?",
       subtitle = "Only words appearing in at least 100 tweets",
       y = "Geometric mean of the Number of retweets")
```
- **? Was wären Kriterien und Strategien für Sie nach denen Sie einen automatisierten Account (bot) in dem Twitter Datensatz einstufen würden?**

- **? Annotieren Sie eine Stichprobe an Tweets hinsichtlich dieser Kriterien. Welche User würden Sie als bots einstufen?**

- **? Wie würden Sie mit diesen Accounts weiter verfahren?**



#### 9. Daten speichern
```{r}
write_csv(tweets, file = "./data/tweets.csv")
```


#### Ausblick und Quellen

Barrie, Christopher & Ho, Justin Chun-ting. (2021). academictwitteR: an R package to access the Twitter Academic Research Product Track v2 API endpoint. *Journal of Open Source Software*, *6*(62), 3272, https://doi.org/10.21105/joss.03272

[Breuer, J. (2022). Demo Twitter-Daten mit R](https://github.com/jobreu/demo-twitter-r)

[Breuer, J. (2022). Twitter linking workshop](https://github.com/jobreu/twitter-linking-workshop-2022)

[Breuer, J., Kohne, J., &  Mohseni, M.R. (2021). Workshop "Automatic Sampling and Analysis of YouTube Comments", GESIS 2021](https://github.com/jobreu/youtube-workshop-gesis-2021)

[Hvitfeldt, E. & Silge, J. (2022). Supervised Machine Learning for Text Analysis in R von ](https://smltar.com/)

[Silge, J. & Robinson, D. (2022). Text Mining with R - A Tidy Approach von ](https://www.tidytextmining.com/)

[Niekler, A. & Wiedemann, G. (2020). Text mining in R for the social sciences and digital humanities von A](https://tm4ss.github.io/docs/)

[Twitter datasets](https://github.com/shaypal5/awesome-twitter-data)
[MetaCorpus of social media corpora](https://github.com/socialmediaie/MetaCorpus)

[Twitter datasets](https://vlo.clarin.eu/search;jsessionid=1AAE84B9F01F93D93D755DAEF7BCE0FF?0&q=twitter)



