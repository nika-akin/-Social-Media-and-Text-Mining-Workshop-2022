---
title: "Social-Media und Text-Mining-Workshop mit R"
output: html_notebook
---
#### Generelle Schritte bei der Textanalyse

A) **Sampling Text**
- Bestehende Archive (z.B. [Reddit Pushift Data](https://files.pushshift.io/reddit/comments/), vgl. auch [Baumgartner et al., 2020](https://cs.paperswithcode.com/paper/the-pushshift-reddit-dataset))
- APIs (The Guardian)
- Web-Scraping

B) **Datenaufbearbeitung**
- Bereinigung
- *Pre-processing*
- Auswahl und Gewichtung von *Features* (Dimensionsreduktion, Filtern von häufigen/ seltenen Wörtern)

C) **Analyse**
- *Dictionary* oder regelbasierte Ansätze
- Supervised Ansätze (z.B. SVM)
- Unsupervised Ansätze (z.B. Topic Modeling)

D) **Validierung**
- Reliabilität


#### Datenzugänge

A) **Screen-Scraping**: 
- Auslesen des HTML-Codes aus dem Browser 
- Parsing und Formatierung 
- Analyse der Daten

B) **API-Access-Points**: 
- Anfragen direkt an die Datenbank senden
- Gateways für den Zugang zu definierten Datentypen für verschiedene Parteien, unabhängig von der Kodierungssprache


##### Application Programming Interfaces (APIs)

- kommuniziert direkt mit der zugrunde liegenden Datenbank
- steuert, welche Informationen zugänglich sind, für wen, wie und in welchen Mengen


**API Anwendungen**:

- Inhalte in andere Anwendungen einbetten
- Bots erstellen (z.B. [Telegram](https://core.telegram.org/bots/api))
- Daten für (Markt-)Forschungszwecke sammeln

Solche Zugangspunkte bestehen beispielsweise für:

- [**Youtube**](https://developers.google.com/youtube/v3) 
-- erlaubt es auf der Grundlage von Keywords nach Inhalten suchen und dem Video, denWiedergabelisten und Nutzeraktivitäten wie Upvoting, Kommentare, Kommentaren, Favorisieren) 

- [**Instagram**](https://github.com/digitalmethodsinitiative/dmi-instascraper/) 
-- erlaubt es Kommentarstrukturen im Zusammenhang mit Postings, Freundschaftsinformationen von Nutzern oder Geolocation zu erhalten

- [**Wikipedia**](https://cran.r-project.org/web/packages/WikipediR/WikipediR.pdf)
-- erlaubt es MediaWiki Revisionen, die mit dem Eintrag verbundene Bearbeitungszusammenfassung, Zeitstempel, Seiteninformationen, Nutzerinformationen zu erhalten

- [**Google Maps**](https://cran.rstudio.com/web/packages/mapsapi/vignettes/intro.html)
-- erlaubt es Koordinaten für Breiten- und Längengrade, Distanzmatrix-Antwort zu erhalten



**Vorteile von API-Harvesting**:

- Keine Interaktion mit HTML-Dateien erforderlich (normalerweise *JSON*-Dateien)
- Zumeist legal (einhalten von Nutzungsbedingungen und Datenschutz- und Urheberrechtsbestimmungen beachten)


**Nachteile des API-Harvesting**:
- Nicht jede Website hat eine API
- Nur die Daten abrufbar, die die API zur Verfügung stellt
- *Rate-Limitations* (z.B. Anzahl der Tweets pro Tag)
- Nutzungsbedingungen und Veränderungen an der API schränken Nutzung ein (z.B. Code-Reprodzierbarkeit, Teilen von Datensätzen)
- Code variiert je nach Platform und Detailierungsgrad der Dokumentation


#### Tool-Übersicht

Unterscheidung nach Anforderungen:

- Graphical User Interface (GUI)
- Art der Daten
- Eignung nur zur Erhebung oder auch zur Aufbereitung und Analyse
- Welche Version der Twitter API wird vorausgesetzt

A. [Twitter Tool List im Wiki des Social Media Observatory am Leibniz-Institut für Medienforschung | Hans-Bredow-Institut (HBI)](https://smo-wiki.leibniz-hbi.de/Twitter-Tools)

B. [Tutorial zu Twitter-Daten](Jürgens, P., & Jungherr, A. (2016). A tutorial for using Twitter data in the social sciences: Data collection, preparation, and analysis. Preparation, and Analysis (January 5, 2016).)

C. [Tools zur Datenerhebung und Analyse von Social-Media-Daten](https://wiki.digitalmethods.net/Dmi/ToolDatabase)

D. Python-basiert: wie z.B. [twarc](https://twarc-project.readthedocs.io/en/latest/)



#### Academic Twitter Access Point
Um die folgenden Code-Beispiele ausführen zu können, wird [Academic Research Access](https://developer.twitter.com/en/products/twitter-api/academic-research) für die Twitter API v2 benötigt. Zum Sampling von Twitter Daten nutzt man das `R`-Paket [`academictwitteR`](https://cran.r-project.org/web/packages/academictwitteR/academictwitteR.pdf).

In dem Rahmen benötigt man den sogenannten [Bearer Token](https://developer.twitter.com/en/docs/authentication/oauth-2-0/bearer-tokens). In dieser [Vignette](https://cran.r-project.org/web/packages/academictwitteR/vignettes/academictwitteR-auth.html) im `academictwitteR`-Paket wird der Vorgang wie man Zugang zur Twitter API erhält erklärt.

(Eine sehr umfangreiche Dokumentation zum Sampling über die **Twitter API v1.1** bieten das Pakete `rtweet`.

 
#### Datenquellen

1: [Kaggle Data Dump, Squid Game, "tweets_v8.csv"]: (https://www.kaggle.com/datasets/deepcontractor/squid-game-netflix-twitter-data)

2: [Kaggle Data Dump, Ukraine, "en_tweets_sample.pkl"]:
(https://www.kaggle.com/code/josbenard/prepare-datasets/data)


#### `R`-Pakete
```{r, eval=FALSE}
library(academictwitteR)    # Sampling
library(tidyverse)          # Data Wrangling, Aufbearbeitung
library(quanteda)           # Text Mining
library(tidytext)           # Text Mining
library(textclean)          # Normalisierung und Pre-Processing 
library(quanteda.textstats)
library(quanteda.textplots)
```
#### Sampling
**Quellen zur Academic Twitter API**:
- https://developer.twitter.com/en/use-cases/do-research/academic-research/resources

- https://cran.r-project.org/web/packages/academictwitteR/vignettes/academictwitteR-tidy.html

- https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query#list

**Authentication**
```{r}
#Einlesen API Credentials
oautCred <- read.delim(file = "oautCred.txt", header = TRUE, sep = ",") %>% unlist()

#Einlesen "Bearer Token" in die Environment
Sys.setenv(TWITTER_BEARER = unname(oautCred["bearer"]))
```
**Twitter-Query**
```{r}
#Tweets Abfrage (basierend auf academic researcher developer account)
#Tweet Suchabfrage (beinhaltet "#hashtags, ggf. "keywords", Sprache)

get_all_tweets(
  query = '(#Ukraine) -is:retweet',                   #tweet Suchabfrage
               n = Inf,                               #max tweet nr 
               start_tweets = "2022-08-19T00:00:00Z", #Start-Datum
               end_tweets = "2022-08-19T23:59:59Z",   #End-Datum
               file = "ukraine_tweets",               #Dateiname 
               data_path = "ukraine_data/",           #Pfad zur Datei
               bind_tweets = TRUE,                    #Format als Tabelle
               )


#Entfernen der API Credentials
rm(oautCred)

```

```{r}
tweets <- bind_tweets(data_path = "ukraine_data/", output_format = "tidy")
```
**Variablen Überblick**

```{r}
glimpse(tweets)
```
##### Anteil der Sprachen
Wie ist die Verteilung der Sprache über die Accounts?
```{r}
table(tweets$language)
```
Anzahl der Tweets täglich
```{r}
ggplot(data = tweets,
      aes(x = tweetcreatedts)) +
      geom_histogram(bins = 57, color = "black", fill = NA)+
      ggtitle(label = "Tägliche Tweetanzahl")
```
**Tweets mit den meisten Favorites und Retweets**
Welche Tweets haben die meisten Favorites und Retweets bekommen?

```{r}
tweets %>% 
  arrange(-favorite_count) %>% 
  select(text, usercreatedts, favorite_count) %>% 
  head(10)
```

```{r}
tweets %>% 
  arrange(-retweetcount) %>% 
  select(text, usercreatedts, retweetcount) %>% 
  head(10)
```

#### Grundlagen Text Mining in a Nutshell
A.**Document** = Sammlung von Zeichenfolgen und ihre Metadaten

B.**Corpus** = Sammlung von *Documents* 

C.**Tokens** = kleinste Bedeutungseinheit (meist Wörter)

D.**Vocabulary** = Sammlung eindeutiger Wörter eines *Corpus*

E.**D**ocument-**F**eature-**M**atrix oder **D**ocument-**T**erm-**M**atrix = dies ist eine Matrix, in der die Zeilen die Dokumente, die Spalten die Ausdrücke und die Zellen die Häufigkeit der Ausdrücke in den einzelnen Dokumenten angeben, d.h.: Matrix mit *n* = Anzahl der Document-Zeilen und *m* = Größe der Vocabulary Spalten, basiert auf der **bag-of-words**-Annahme, das die Reihenfolge der Wörter und die Syntax ignoriert. 



##### Pre-processing
A. Tokenisation

B. Stopwords entfernen

C. String operations (Interpunktion, URL normalisieren)

Die Reihenfolge der Anwendungsschritte ist entscheidend und sollte durch die Forschungsfrage geleitet sein.

```{r}
# Bereinigen von @-Symbol
tweets$text <- gsub("@\\w+", "", tweets$text)

# Bereinigen der Interpuntion
tweets$text<- gsub("[[:punct:]]", "", tweets$text)

# bereinigen von Nummern
tweets$text <- gsub("[[:digit:]]", "", tweets$text)

# Bereinigen von Bildern
tweets$text<- gsub("pictwitter\\w+ *", "", tweets$text)
```

Bereinigen von HTML-Notation
```{r}
remove_html <- "&amp;|&lt;|&gt;" #&lt und &gt stehen für < und > und &amp für &

tweets_en <- tweets %>% 
  filter(language == "en") %>% 
  select(tweetid, text, username) %>% 
  mutate(text = str_remove_all(text, remove_html))
```

Mit [`textclean`](https://github.com/trinker/textclean) kann weitere Harmonisierung durchgeführt werden, z.B. Ersetzen von Emojis durch Wortäquivalente.

```{r}
tweets$text <- replace_emoji(tweets$text)
```
Oder Normalisieren von Wortverlängerungen (Elongation, z.B."Whyyyy")
```{r}
tweets$text <- replace_word_elongation(tweets$text)
```

Aus dem data frame kann man ein *Corpus-Objekt* erstellen, d.h. eine Sammlung von *Dokumenten* (Tweets) und deren *Metadaten*.

```{r}
tweets_en_corpus <- corpus(tweets_en,
                           docid_field = "tweetid",
                           text_field = "text")

summary(tweets_en_corpus)
```


In `quanteda` kann man Text in die kleinsten Bedeutungssegmente (**tokens**) aufteilen durch das Entfernen von Trennzeichen (vgl. dazu in `tidytext` `unnest_tokens(word, text)`. Zudem entfernen wir bestimmte Zeichentypen sowie sogenannte **Stopwords** (d.h., grammatikalische Wörter, die je nach Anwendung wenig semantische Bedeutung tragen). Vgl. (`stopwords()`)

```{r}
tweets_en_tokens <- tokens(tweets_en_corpus,
                           remove_punct = TRUE,
                           remove_numbers = TRUE,
                           remove_symbols = TRUE,
                           remove_url = TRUE) %>% 
  tokens_tolower() %>% 
  tokens_remove(stopwords("english"))
```

**?** Versuchen Sie es mit einer eigenen custom stopwords Liste.


Mit **keyword-in-context** kann man eine Konkordanz-Ansicht erhalten.
```{r}
kw_ukraine <- kwic(tweets_en_tokens, pattern =  "ukrain*", window = 3)
view(kw_ukraine, 10)
```

Des Weiteren kann man in `quanteda`aus dem Token-Objekt eine **Document-Feature Matrix** (DFM) erstellen, (Wörter werden als Features eines textbasierten Datensatzes behandelt).
```{r}
tweets_en_dfm <- dfm(tweets_en_tokens) %>% 
  print()

topfeatures(tweets_en_dfm, 10)
```

Absolute Worthäufigkeiten mit `quanteda.textstats`

```{r}
term_freq_en <- textstat_frequency(tweets_en_dfm)
head(term_freq_en, n = 10)
```

**?** Wenn Sie sich das Ergebnis ansehen, gibt es dann korpusspezifische Begriffe, die ebenfalls als *Stopwords* betrachtet werden sollten?

**?** Wie hoch ist der Anteil der Begriffe am gesamten Vokabular, die nur einmal im Korpus vorkommen?


Lexikalische Diversität

```{r}
lexdiv_tweets <- textstat_lexdiv(tweets_en_dfm)

plot(lexdiv_tweets$TTR, type = "l", xaxt = "n", xlab = NULL, ylab = "TTR")
grid()
axis(1, at = seq_len(nrow(lexdiv_tweets)), labels = tweets_en_dfm$username)

```

Umwandeln einer DFM in eine **Feature Co-occurrence Matrix (FCM)** (z.B. für Word Embeddings) 
```{r}
tweets_en_fcm <- fcm(tweets_en_dfm)

topfeatures(tweets_en_fcm) # Häufigsten Co-occurences
```


Plot der Feature Co-Coccurences der FCM mit `quanteda.textplot`
```{r}
feat <- names(topfeatures(tweets_en_fcm, 50))

fcmat_tweets_select <- fcm_select(tweets_en_fcm, pattern = feat, selection = "keep")
dim(tweets_en_fcm)

size <- log(colSums(dfm_select(tweets_en_fcm, feat, selection = "keep")))

set.seed(144)
textplot_network(fcmat_tweets_select, min_freq = 0.7, vertex_size = size / max(size) * 3)

```


#### Daten speichern
```{r}
write_csv(tweets, file = "./data/tweets.csv")
```


#### Ausblick und Quellen

Barrie, Christopher & Ho, Justin Chun-ting. (2021). academictwitteR: an R package to access the Twitter Academic Research Product Track v2 API endpoint. *Journal of Open Source Software*, *6*(62), 3272, https://doi.org/10.21105/joss.03272

[Breuer, J. (2022). Demo Twitter-Daten mit R](https://github.com/jobreu/demo-twitter-r)

[Breuer, J. (2022). Twitter linking workshop](https://github.com/jobreu/twitter-linking-workshop-2022)

[Breuer, J., Kohne, J., &  Mohseni, M.R. (2021). Workshop "Automatic Sampling and Analysis of YouTube Comments", GESIS 2021](https://github.com/jobreu/youtube-workshop-gesis-2021)

[Hvitfeldt, E. & Silge, J. (2022). Supervised Machine Learning for Text Analysis in R von ](https://smltar.com/)

[Silge, J. & Robinson, D. (2022). Text Mining with R - A Tidy Approach von ](https://www.tidytextmining.com/)

[Niekler, A. & Wiedemann, G. (2020). Text mining in R for the social sciences and digital humanities von A](https://tm4ss.github.io/docs/)

[Twitter datasets](https://github.com/shaypal5/awesome-twitter-data)
[MetaCorpus of social media corpora](https://github.com/socialmediaie/MetaCorpus)

[Twitter datasets](https://vlo.clarin.eu/search;jsessionid=1AAE84B9F01F93D93D755DAEF7BCE0FF?0&q=twitter)



